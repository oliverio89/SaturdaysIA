{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **AISaturdays Ocean Proximity Challenge - Solución**\n",
    "\n",
    "Bienvenido al reto de **AISaturdays** de Inteligencia Artificial del curso de Deep Learning.\n",
    "\n",
    "En este ejercicio trataremos de predecir la proximidad del océano con respecto a un conjunto de viviendas.\n",
    "\n",
    "**Enlaces:**\n",
    "- AISaturdays Deep Learning [temario clase 1](https://app.eduflow.com/courses/d4d36f90-f65e-4ede-b8f3-16f498ce3a33/flows/98016aaa-4e29-428f-a8f9-d5d9477d7274/activities/d9f00e62-0403-4a21-bdae-c6a3ea2984e5).\n",
    "- Ejercicio Kaggle: https://www.kaggle.com/camnugent/california-housing-prices\n",
    "\n",
    "**Instrucciones:**\n",
    "- Se usará el lenguaje de programación Python 3.\n",
    "- Se usarán las librerías de python: *Pandas, MatPlotLib, Numpy, Scikit-learn, Keras,TensorFlow*.\n",
    "\n",
    "**Mediante este ejercicio, aprenderemos:**\n",
    "- Entender y ejecutar los NoteBooks con Python.\n",
    "- Ser capaz de utilizar funciones de Python y librerías adicionales.\n",
    "- Dataset:\n",
    " - Obtener el dataset y previsualizar la información del dataset.\n",
    " - Limpiar y normalizar la información del dataset.\n",
    " - Representar y analizar la información del dataset.\n",
    "- Crear y entender el concepto de \"*Conjunto de datos de entrenamiento*\" y \"*Conjuntos de datos de test*\"\n",
    "- Crear y entender el concepto de \"*Modelo de redes neuronales*\" para procesar los datos,  predecir y obtener conclusiones.\n",
    "- Mejorar la predicción y evitar overfitting.\n",
    "\n",
    "\n",
    "¡Empecemos!"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "cVtZXn9Yq-d-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#1. Importación de librerías\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "gG-uG10H7MJP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Importaremos* las herramientas que vamos a utilizar.\n",
    "Para ejecutar cada celda(caja), pulsa el play de la izquierda o **Ctrl + Enter**."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Gb23ryIM7O6F"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "ck6EgsF4QM20",
    "outputId": "cdabb583-b72d-4067-a168-e542a24f46fe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#2. Dataset\n",
    "![texto alternativo](https://i.imgur.com/TMvhIbd.png)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "mjLIB37q7YfV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtener el dataset y previsualizar la información del dataset."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "MOduCoeM7gJM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "¡Nuestra tabla! Para poder visualizar y limpiar los datos convertiremos el archivo CSV.\n",
    "\n",
    "Los datos se almacenarán en un dataframe llamado **df**."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "4z2qf6qV7jHe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Sólo con una línea de código\n",
    "df = pd.read_csv(\"housing.csv\")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D59xF6NcVkLh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Tu turno. Muestra los primeros valores del archivo CSV para hacernos una idea del contenido de la tabla con el código: df.head()\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Ip5Tr_yO82v1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Sólo con una línea de código\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "hE1nZKr-85Z0",
    "outputId": "9ccfcd77-6bfd-494a-de2a-ecdc23f68a24"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables\n",
    "\n",
    "* **longitude:** Valor longitude de la coordenada.\n",
    "* **latitude:** Valor latitude de la coordenada.\n",
    "* **housing_median_age:** Edad media de las viviendas de esta zona.\n",
    "* **total_rooms:** Total de habitaciones.\n",
    "* **total_bedrooms:** Total de camas.\n",
    "* **population:** Población en esta zona. Importante denotar que también es un valor total.\n",
    "* **households:** Viviendas en esta zona. Importante denotar que también es un valor total.\n",
    "* **median_income:** Salario medio de las personas de esta zona.\n",
    "* **median_house_value:** Valor medio de la vivienda de esta zona.\n",
    "* **ocean_proximity:**  ¡El resultado! Significa la proximidad del océano con respecto a las viviendas de esta zona. Si nos fijamos, este campo contiene valores de cadena (etiquetas) para determinar la proximidad.\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "vzU3111m-MV2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analizar dataset:\n",
    "\n",
    "Para entender la distribución de los datos, vamos a observar:\n",
    "\n",
    "- La cantidad de datos (count)\n",
    "- La distribución de los datos mediante cuartiles (25%,50%,75%)\n",
    "- Media de los datos (mean)\n",
    "- Mínimos y máximos (min, max)\n",
    "- Varianza (std)\n",
    "- ..."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Dek_DLDHDqwP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Utiliza la función .describe() para analizar la distribución de los datos\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "AzQbGrCyDt8N"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Sólo con una línea de código\n",
    "df.describe()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "Bw4pU84Ocst5",
    "outputId": "96ee3176-da9a-4fad-bfff-dc12e8d946b6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualicemos el contenido para hacernos una idea de la distribución de las viviendas por población y precio.\n",
    "\n",
    "*   Es la zona de California, y al suroeste donde terminan los círculos, comieza el océano.\n",
    "![California](https://california.azureedge.net/cdt/CAgovPortal/images/Uploads/menu-living.jpg)\n",
    "*   El precio se indica con color rojo para las viviendas más caras.\n",
    "*   Las viviendas más cercanas al océano se encuentran por lo tanto en la costa de California.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "EctvUGXza5wt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ejecutar el siguiente código\n",
    "plt.figure(figsize=(10,7))\n",
    "plotter = df.copy()\n",
    "plotter.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=plotter[\"population\"]/100, label=\"population\", figsize=(15,8), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),colorbar=True, )\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Limpiar y normalizar la información del dataset\n",
    "![texto alternativo](https://i.imgur.com/8u4xTI7.png)\n",
    "\n",
    "En este apartado se pretende corregir y mejorar información en el dataset que pueda faltar, sea inconsistente o no sea correcta.\n",
    "\n",
    "Este paso es necesario para mejorar cualquier predicción futura que se realice con el dataset.\n",
    "\n",
    "Existen diferentes técnicas para corregir y mejorar la calidad e integridad de información de nuestro dataset. \n",
    "\n",
    "Como futura lectura, recomendamos el siguiente enlace con [técnicas de mejora del dato](https://www.kaggle.com/pavansanagapati/simple-tutorial-how-to-handle-missing-data).\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "EgM114S_PXCA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Muestra el sumatorio de los valores nulos del dataset: df.isnull()...\n",
    "[Documentación](https://datascience.stackexchange.com/questions/12645/how-to-count-the-number-of-missing-values-in-each-row-in-pandas-dataframe)\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "pPv3a-JOHYRt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Sólo con una línea de código\n",
    "df.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "SSVCQSO4WL23",
    "outputId": "08c138a6-12fd-4f55-84f1-85419e1d1f73"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Elimina las filas que tengan valores a 0: df.dropna(...) [Documentación](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "4_G5wmrMHnYE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Sólo con una línea de código\n",
    "df.dropna(inplace=True)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yq-fw6ZfWTtH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analizar relaciones de la información del dataset"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "ubrT1Kc0b3eH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. ¿Cual de estas variables está relacionada con la proxímidad del océano y cúal no? ¿Están relacionadas entre sí? **Explícalo a continuación**\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "s3e4MXHDcvhF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A mayor cercanía de la costa, mayor valor.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "7u7rso4_hGpP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "chgdJhO0hh0o"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Crear una matriz de correlación visual. [Pista](https://datascienceplus.com/visualize-correlation-matrices-in-python/)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "6onWAUyycrGG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Crea una matriz de correlación visual usando .corr()\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "2CuoeTlgclvF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Dos líneas de código\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "6nM77XsEWZmH",
    "outputId": "58bf977b-00b5-4a77-c498-620bce04db69"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "! Fíjate en todas posibles feature de la tabla de arriba que indican \"**valores  totales**\". (Pista: algunas de ellas no llevan el prefijo *total_* )\n",
    "\n",
    "Estas features de totales nos indican los valores en una zona concreta de un grupo de viviendas. Para mejorar la predicción de nuestro modelo, nos conviene obtener los valores individuales por vivienda.\n",
    "\n",
    "Los datasets no suelen ser perfectos, hay datos incompletos e incorrectos que reducen la eficiencia del modelo. Por ello necesitamos limpiarlo. [Pista](https://new.paradigmadigital.com/wp-content/uploads/2019/02/Pandas_cheatsheet.pdf)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "w8j-64Y2QxrH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. ¿Qué features están agrupadas por totales? ¿Con qué feature se agrupan y tienen más relación? (Pista: hay tres)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "2GnJ_B3WS4p-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "total_rooms,total_bedrooms y population"
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DHqkBwPXjud"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. De las feature agrupadas por totales, obtén las feature invididuales dividiéndolas por las que las agrupa, y añádelas al dataframe\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "GS-W5PIId4jX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Tres líneas de código\n",
    "df[\"rooms_per_household\"] = df[\"total_rooms\"]/df[\"households\"]\n",
    "df[\"bedrooms_per_household\"] = df[\"total_bedrooms\"]/df[\"households\"]\n",
    "df[\"population_per_household\"] = df[\"population\"]/df[\"households\"]"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALL9uxc_YIbF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. Elimina del dataset las feature de totales agrupadas\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "MxP6J_UaeZ31"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Sólo con una línea de código\n",
    "df.drop(['total_rooms','total_bedrooms','population','households'],axis=1,inplace=True)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2CD73liWeYxO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Crea de nuevo una matriz de correlación visual usando .corr() para ver las nuevas feature añadidas y su correlación\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "tTcm5qfQiIz1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Dos líneas de código\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "kUQc3-UIYbe_",
    "outputId": "d5c25e27-8a6e-4456-df1b-25f2240ee782"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "! Fíjate como ahora al haber procesado y quitado las feature de totales, hemos eliminado algunas correlaciones y la matriz nos muestra correlaciones más coherentes que nos ayudarán a mejorar la predicción de nuestro modelo.\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "sLUGV34FiX3l"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "13. Obtén los valores únicos de la feature de la que queremos obtener la predicción (ocean_proximity)\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "t1Qe2wrBzZI2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Dos líneas de código\n",
    "df.ocean_proximity = df.ocean_proximity.astype('category')\n",
    "df.ocean_proximity.unique()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v1AvID2eyfmY",
    "outputId": "53a3e4d2-a222-4965-db51-f63f9ba2ae64"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*14*. Explica qué ves en los valores anteriores. \n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "TCOAsvqSzpL4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*   Explica aquí qué ves en los valores anteriores que deba mejorarse\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "awXQJZVj0AHd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "VLu-zYlLz-PV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "15. Mejora el dataframe con el análisis anterior\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "0edpZ4qT0Jdd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Sólo con una línea de código\n",
    "df = df[df['ocean_proximity'] != 'ISLAND']"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OI5kVUcLzSBG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizar las feature para nuestro modelo\n",
    "\n",
    "![texto alternativo](https://i.imgur.com/o7YvGRe.png)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "_MohTJvooR_e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Ahora deberemos proceder a normalizar el resto de variables para que la escala de valores entre ellas sea similar y facilite el procesamiento a nuestro modelo y se logre mejorar la predicción (*Feature Scaling*).\n",
    "Este paso es fundamental antes de entrenar nuestro modelo.\n",
    "\n",
    "Puesto que la **función de activación** de nuestro modelo será '**softmax**', cuyos valores son de una escala entre -1 a 1, debemos normalizar las feature en este intervalo. [Pista](https://books.google.es/books?id=M5RRDwAAQBAJ&pg=PA137&lpg=PA137&dq=scaler.fit_transform+values.reshape&source=bl&ots=zzDuEQUdQR&sig=ACfU3U1NPGvTLq46zc-pJwcexE3wGKHQPw&hl=es&sa=X&ved=2ahUKEwi1qtPZ9MDnAhXdDWMBHc-MB0UQ6AEwDXoECAgQAQ#v=onepage&q=scaler.fit_transform%20values.reshape&f=false)"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "N4AnSiMqmLpH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "16. Normaliza las feature a la escala de softmax y añádelas al dataframe.\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "E8SVkwkKtSy_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Seis líneas de código (cómo mínimo)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "df['longitude'] = scaler.fit_transform(df['longitude'].values.reshape(-1,1))\n",
    "df['latitude'] = scaler.fit_transform(df['latitude'].values.reshape(-1,1))\n",
    "df['housing_median_age'] = scaler.fit_transform(df['housing_median_age'].values.reshape(-1,1))\n",
    "df['median_income'] = scaler.fit_transform(df['median_income'].values.reshape(-1,1))\n",
    "df['median_house_value'] = scaler.fit_transform(df['median_house_value'].values.reshape(-1,1))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxWDrBS5YlHY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "17. Utiliza la función .describe() para analizar la distribución de los datos\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "L8uHwFQQxnMA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Sólo con una línea de código\n",
    "df.describe(include='all')"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "87Fa9SY-b0pc",
    "outputId": "4f898bff-28f5-4618-cc58-f485bfaa9051"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Modelo redes neuronales\n",
    "\n",
    "El modelo de [redes neuronales](https://pathmind.com/wiki/neural-network) es una serie de algoritmos que logran reconocer patrones y correlaciones en un conjunto de datos a través de un proceso que simula el funcionamiento del cerebro con la finalidad de determinar predicciones que puedan interesarnos.\n",
    "\n",
    "En Deep Learning uno de los usos más frecuentes del modelo de redes neuronales es el caso de análisis de datos en [aprendizaje supervisado](https://es.wikipedia.org/wiki/Aprendizaje_supervisado).\n",
    "\n",
    "\n",
    "\n",
    "**Ejemplo genérico de Red Neuronal**\n",
    "\n",
    "![texto alternativo](https://pathmind.com/images/wiki/perceptron_node.png)\n",
    "\n",
    "**Ejemplo de Red Neuronal Clasificador con función de activación SoftMax**\n",
    "\n",
    "![Modelo](https://i.imgur.com/wcOwh5O.png)\n",
    "\n",
    "\n",
    "**Ejemplo**\n",
    "- Dado un conjunto de emails, determinar cuál es spam y cuál no.\n",
    "- Dado un conjunto de imágenes de animales, determinar cuáles son perros.\n",
    "- Detectar partes del cuerpo en imágenes o gestos.\n",
    "- Detectar voces, transcripciones, textos..\n",
    "\n",
    "**Objetivo**\n",
    "- En este reto, queremos lograr **clasificar** aquellas viviendas según a la cercanía que estén del océano. Es un ejercicio de **clasificación**.\n",
    "- A continuación puede visualizarse la diferencia entre \"clasificación\" y \"regresión\"\n",
    "![Clasificación vs regresión](https://res.cloudinary.com/practicaldev/image/fetch/s--c4Lfzdwy--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/mjshszqx4fj22hs12vfn.png)\n",
    "\n",
    "**Implementación**\n",
    "- Crear un conjunto de datos de entrenamiento y un conjunto de datos de test\n",
    "- Crear un modelo de red neuronal con Keras:\n",
    "  - Tipo de modelo: [tipo de algoritmo](https://www.javatpoint.com/classification-algorithm-in-machine-learning): *Sequential*\n",
    "  - Añadir 7 capas Dense.\n",
    "  - La primera capa densa llevará seteada el número de features con las que se entrena el modelo, sin contar con la que queremos averiguar.\n",
    "  - Las seis primeras capas llevarán la función de activación 'relu'.\n",
    "  - La última capa llevará la función de activación 'softmax'.\n",
    "- Compilar el modelo con:\n",
    "  - Los \"datos de entrenamiento\" y de las \"features\" que queremos tener en cuenta para el entrenamiento.\n",
    "  - Función de activación: 'softmax'. [Pista](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax?hl=es-419)\n",
    "  - Función de pérdida: 'categorical_crossentropy'. [Pista](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy)\n",
    "  - Función de optimización: 'adam'. [Pista](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "- Entrenar el modelo con los \"datos de entrenamiento\" y de las \"features\" que queremos tener en cuenta para el entrenamiento y los datos de validación.\n",
    "- Determinar las conclusiones de los resultados obtenidos en el modelo."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "fr-VjmbA0jxY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creación del modelo de redes neuronales"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "k-hl6T_NQM2f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "18. Crea el set de datos de entrenamiento y validación necesarios para nuestro modelo\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "3X6FQOUGLxgQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Crea X , que sea igual al dataframe menos la feature que queremos averiguar.\n",
    "# Sólo una línea de código\n",
    "X = df.drop([\"ocean_proximity\"],axis = 1)\n",
    "\n",
    "# Crea y , con un LabelEncoder para pasar a enteros los valores que tiene la feature que queremos averiguar\n",
    "# Dos lineas de código\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y = encoder.fit_transform( df.ocean_proximity)\n",
    "\n",
    "# Crea el set de datos de entrenamiento y validación.\n",
    "# Sólo una línea de código\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Convierte y_train y y_test al formato que se necesita par entrenar nuestro modelo con: np_utils.to_categorical\n",
    "num_classes = len(np.unique(y))\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIRENct3cnwq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "19. Crea y compila el modelo como se indican en los pasos de implementación descritos anteriormente\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "DFUhoLI4ObHO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Importar librerías keras\n",
    "import keras.metrics as metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, restore_best_weights=True,patience=10)\n",
    "# Crear el modelo secuencial\n",
    "# Sólo una línea de código\n",
    "model = Sequential()\n",
    "# Añadir 7 capas Dense con sus propiedades como se indica en la implementación\n",
    "# Siete líneas de código\n",
    "model.add(Dense(500, input_dim=8, activation= 'relu'))\n",
    "model.add(Dense(300,activation = 'relu'))\n",
    "model.add(Dense(300,activation = 'relu'))\n",
    "model.add(Dense(300,activation = 'relu'))\n",
    "model.add(Dense(100,activation = 'relu'))\n",
    "model.add(Dense(100,activation = 'relu'))\n",
    "model.add(Dense(100,activation = 'relu'))\n",
    "model.add(Dense(4,activation = 'softmax'))\n",
    "\n",
    "\n",
    "# Compilar el modelo como se indica en la implementacióbn\n",
    "# Sólo una línea de código\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "NTaD3tiaeDfq",
    "outputId": "b06a3858-8a67-4a02-ea2a-9ab58fe9b7ff"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "20. Explica los siguientes conceptos\n",
    "---\n",
    "- ¿Por qué se ha utilizado el tipo de modelo 'Sequential'\n",
    "- ¿Qué valor debe tener input_dim y que feature no se debe contabilizar?\n",
    "- ¿Qué función de activación se ha seteado en la última capa?\n",
    "- ¿Por qué se ha elegido esa función de activación en la última capa?\n",
    "- ¿Por qué se ha elegido 'categorical_crossentropy' en la función de pérdida?\n",
    "- ¿Qué significa la terminología de función de optimización?\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "viby-FmRVLI4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Escribe aquí las respuestas"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "FATxekmfYMaI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Visualizar el resultado de la compilación del modelo\n",
    "# Sólo una línea de código\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "-vqblbMvVAu2",
    "outputId": "0dd333b3-653e-4c32-8394-0ccdb9e4f4b6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "21. Entrena finalmente el modelo con los parámetros necesarios: X_train, Y_train, epochs=???, batch_size=???, validation_data=(X_test,Y_test)\n",
    "---\n",
    "- El set de entrenamiento Y_train debe esta previamente convertido al formato que necesita el modelo. No se debe usar directamente el obtenido de train_test_split(...)\n",
    "- Prueba con diferentes epochs (iteraciones): 10, 100, 500, 1000.\n",
    "- Prueba con diferentes batch_size: 10, 200, 1000, 100000.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "t5l3451HQVRE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#Usa este EarlyStopping para conseguir un mejor modelo\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, restore_best_weights=True,patience=10)\n",
    "# Sólo con una línea de código. Recuerda guardar el historial de entrenamiento para los siguientes pasos.\n",
    "history = model.fit(X_train,Y_train,validation_split = 0.25,batch_size=200,epochs=150,callbacks = [es])"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7r8GKmhce-9j",
    "outputId": "e9f46c8c-6f5d-49be-b0ec-67d5c174c382"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Evalua el modelo con los datos de X_test y Y_test\n",
    "results = model.evaluate(X_test,Y_test)\n",
    "results"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rMbmOO10czow",
    "outputId": "7b4af9f7-85da-4929-e27c-1b24036dfe54"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "22. Explica qué ocurre con diferentes epochs y batch_size\n",
    "---\n",
    "- ¿Qué ocurre con demasiados epochs? [Pista](#https://stats.stackexchange.com/questions/384593/why-too-many-epochs-will-cause-overfitting)\n",
    "- ¿Qué ocurre con un valor demasiado grande de batch_size?\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "eq3QEfAVSUMG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Escribe aquí las respuestas:\n",
    "\n",
    "Demasiadas epochs generan overfitting.\n",
    "Batch sizes demasiado grandes son muy rápidas per epoch pero también generan overfiting.\n",
    "Batch sizes demasiado pequeñas tardan demasiado."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Wg5bi3ICYT09"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Función auxiliar para ver la gráfica de la ejecución del modelo"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "dQAW0stCSr1A"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# FUNCIÓN AUXILIAR, EJECUTAR, NO MODIFICAR.\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # Summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['accuracy'])+1),model_history.history['accuracy'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_accuracy'])+1),model_history.history['val_accuracy'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zwjUClC35B8f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "23. Imprime las gráficas de entrenamiento del modelo\n",
    "---\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Xc7Gf8yfSzFN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Sólo con una línea de código\n",
    "plot_model_history(history)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "bwOd1JEUYidc",
    "outputId": "052f90b3-78fd-435c-8108-2568f1bc7b63"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "24. Analiza y explica qué indica la gráfica del modelo\n",
    "---\n",
    "- Explica la gráfica de Accuracy\n",
    "- Explica la gráfica de Loss\n",
    "- *Escribe aquí las respuestas"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "dUv2TozzS6u-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Mejorar la predicción\n",
    "\n",
    "Mejorar la predicción y resultado de nuestro modelo es posible:\n",
    "\n",
    "*   Aumentar el dataset o mejorar la calidad de los datos existentes.\n",
    "*   **Optimizar hyperparámetros** de nuestro conjunto de datos y de nuestro modelo.\n",
    "\n",
    "A continuación se pretende mejorar la predicción mediante la \"*optimización de hyperparámetros*\"."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "U75YZYw-TgtO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "25. Intenta mejorar la predicción del modelo anterior modificando los hyperparámetros de la creación del set de datos de entrenamiento y validación: random_state\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "XtnEv27XTzYv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Escribe aquí el código"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oktpkQYUUAXs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Explica aquí que resultados has obtenido con el intento de mejora de la predicción del modelo.\n"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "DOhRpAxIUFJG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "26. Intenta mejorar la predicción del modelo anterior modificando los hyperparámetros de la creación del modelo: Nº de parámetros de las capas Densas\n",
    "---"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "plv5FFxoUMq0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Mas capas ≠ Mas accuracy\n",
    "Utilizar Early stopping permite llegar a buenas accuracys sin overfitting pero puede caer en local minimums si no se utiliza una patience suficientemente alta.\n",
    "Lo mejor es ir de mas a menos capas si quieres alcanzar algún resultado."
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUERKXDLUzw3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Explica aquí que resultados has obtenido con el intento de mejora de la predicción del modelo."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "HnKpcVxOU1ed"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_MohTJvooR_e"
   ],
   "name": "S1_House_challenge_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "debd7f09b077943c30761037e822b136a322b4ca98aecc4298d815b1829f859f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('AISat': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}